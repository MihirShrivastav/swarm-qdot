# Scalable Multi-State Neural Block-Ritz Solver for 2D Quantum Dot Systems

This repository contains an academic research implementation for solving the time-independent Schrodinger eigenproblem for a single electron in a 2D confinement potential, with emphasis on obtaining the ground state and multiple excited states in one training run.

The project targets quantum-dot and double-well style potentials and is designed to produce reproducible numerical artifacts (energies, wavefunctions, diagnostics, plots) for scientific analysis.

## Why This Project Exists

Classical grid solvers (finite-difference and finite-element) remain strong baselines, but they can become expensive when:

- high spatial resolution is required,
- potential landscapes are changed repeatedly,
- multiple low-lying eigenstates are needed,
- larger parameter sweeps are desired.

Traditional neural approaches often train one model per state (or sequentially deflate), which scales poorly and introduces state bookkeeping complexity.

This project explores a different route:

- learn a shared neural basis subspace once,
- solve a small projected eigenproblem inside that subspace,
- extract many low-energy states at once.

## Core Research Idea

The central innovation here is a Block-Ritz neural variational solver:

1. A neural network outputs `M` basis functions simultaneously.
2. The code assembles projected overlap and Hamiltonian matrices (`S`, `H`) via weak-form quadrature.
3. It solves `H c = S c Lambda`.
4. It takes the lowest `K <= M` Ritz pairs as the predicted eigenstates.

This avoids sequential per-state training and enables multi-eigenpair prediction within a single training regime.

## Physics Problem

We solve (dimensionless form):

`H psi = E psi`, with `H = -nabla^2 + V(x, y)`, over a rectangular domain `[-X, X] x [-Y, Y]`.

Physical energy conversion:

`E_meV = E_dimless * E0_meV`, where `E0_meV = hbar^2 / (2 m* L0^2)`.

Current material defaults correspond to GaAs effective mass settings.

## Potentials and Physical Controls

The current code supports:

- `biquadratic_dqd` (double-well and double-dot style)
- `gaussian` (single anisotropic Gaussian landscape)

### Interpretable biquadratic controls

For `biquadratic_dqd`, three control modes are available:

1. `raw`:
   - direct coefficients (`c4`, `c2y`)
2. `targets`:
   - physical local spacings (`hbar_omega_x_meV`, `hbar_omega_y_meV`)
   - coefficients are derived automatically from material scaling
3. `barrier`:
   - direct barrier target (`barrier_meV`) plus y-confinement target

Resolved physical and derived parameters are saved to:

- `reports/potential_report.json`

## Methodology

### Variational weak form

The implementation uses weak-form energy (first derivatives) instead of direct Laplacians:

- improves speed and stability in autodiff,
- reduces high-order derivative noise.

### Projected eigensolve

At each training step:

1. evaluate basis values `Phi` and basis gradients `grad(Phi)`,
2. assemble `S` and `H`,
3. solve generalized eigensystem,
4. minimize sum of the lowest `K` Ritz values.

### Losses

Current loss components:

- eigen-sum objective (primary),
- overlap conditioning penalty for `S` stability.

### Sampling strategy

Training quadrature is configurable:

- `fixed_grid`
- `jittered_grid` (default, anti-aliasing)
- `monte_carlo`

Stochastic sampling reduces fixed-grid exploitation and helps avoid spiky variational cheats.

### Precision strategy

Training precision is configurable:

- `float32`
- `float64` (default for robustness of eigensolve and training)

### Learning-rate schedule

A plateau scheduler (`ReduceLROnPlateau`) is integrated with an EMA-smoothed monitor to reduce LR when eigsum stalls.

### Post-training deterministic evaluation

Even when training uses stochastic or jittered sampling, final artifacts are always generated by re-evaluating on a fixed uniform grid for deterministic reporting.

## Model Architecture

### Block-SIREN basis network

The neural basis is produced by `BlockSIREN`:

- input: `(x, y)`
- hidden: sinusoidal layers (`SineLayer`)
- output: `M` basis channels

Optional components:

- Gaussian envelope for spatial decay (`exp(-alpha x^2 - beta y^2)`),
- parity-enforced basis construction for symmetric potentials.

## Training Regime

Each run:

1. loads and validates config,
2. resolves potential parameters into concrete coefficients,
3. trains with Adam (plus optional gradient clipping and LR decay),
4. logs step metrics,
5. saves best and final checkpoints,
6. evaluates final states on fixed grid,
7. writes arrays, reports, and plots.

Checkpoint outputs:

- `checkpoints/model_best.pt`
- `checkpoints/model_final.pt`

## Why This Instead of Purely Numerical Solving

This repository does not claim to replace classical solvers universally.

It investigates a research hypothesis:

- a learned neural subspace plus projected eigensolve can provide a flexible multi-state workflow with strong amortized behavior for repeated studies and parameter sweeps.

The approach is attractive when:

- many low-energy states are needed repeatedly,
- potential families are explored across many runs,
- one wants unified learning-based representations plus matrix-level physics checks.

Classical finite-difference and finite-element methods remain important baselines and should be used for cross-validation.

## Running Experiments

### Single run (default config)

```bash
python -m experiments.run_single_config --experiment-name my_run --seed 42
```

### Single run (custom config)

```bash
python -m experiments.run_single_config --config path/to/config.json --experiment-name my_run --seed 42
```

### Default run with random W&B-style experiment name

```bash
python -m experiments.run_default_random
```

## Run Artifacts

Each run is stored under:

- `results/<experiment-name>/<run-id>/`

Subfolders:

- `logs/`
- `checkpoints/`
- `arrays/`
- `reports/`
- `plots/`

Key reports:

- `energies.json`
- `orthonormality_report.json`
- `potential_report.json`
- `checkpoint_report.json`
- `final_summary.json`

Key arrays:

- `psi_grid.npy`
- `phi_basis.npy`
- `potential_grid.npy`
- `overlap_S.npy`
- `hamiltonian_H.npy`
- `psi_overlap.npy`

Representative plots:

- potential maps (`full`, `focus`, `slice_y0`)
- state density maps and signed wavefunction maps
- training losses, conditioning, energies, timing
- matrix diagnostics and energy spectrum

## Repository Layout

- `experiments/`: run entrypoints
- `physics/`: material scaling and potential models
- `models/`: Block-SIREN and envelope/parity utilities
- `numerics/`: sampling, autodiff helpers, Ritz assembly and solve
- `training/`: training loop and loss definitions
- `utils/`: config, logging, run management, artifacts, plotting
- `tests/`: unit tests
- `docs/`: specs and technical documentation

## Current Scope and Limitations

Current scope:

- single-configuration runs,
- one-electron 2D potentials,
- multi-state extraction via Block-Ritz.

Not yet fully implemented in the training loop:

- full phased optimizer schedule (for example explicit LBFGS polish stage),
- PDE residual polish term integration,
- cross-potential amortized operator learning.

## Reproducibility Notes

- Every run stores a resolved `config.json`.
- Run metadata includes environment and timestamp info.
- Metrics are stored in both human-readable logs and structured JSONL.
- Final artifacts are generated on a deterministic evaluation grid.
